<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>This is my paper title</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="Path to my teaser.jpg"/>
  	<meta property="og:title" content="Creative and Descriptive Paper Title." />
  	<meta property="og:description" content="Paper description." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="Creative and Descriptive Paper Title." />
    <meta property="twitter:description"   content="Paper description." />
    <meta property="twitter:image"         content="Path to my teaser.jpg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZHY384EZP7"></script>
    <script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments);}
              gtag('js', new Date());

              gtag('config', 'G-ZHY384EZP7');
    </script>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
</head>

<body>
<div class="container">
    <div class="title">
        MOVES: Manipulated Objects in Video Enable Segmentation
    </div>

    <div class="venue">
        In Computer Vision and Pattern Recognition 2023
    </div>

    <br><br>

    <div class="author">
        <a href="https://relh.net">Richard E. L. Higgins</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a><sup>1, 2</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>University of Michigan</div>
    <div class="affiliation"><sup>2&nbsp;</sup>New York University</div>

    <br><br>

    <div class="links"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Higgins_MOVES_Manipulated_Objects_in_Video_Enable_Segmentation_CVPR_2023_paper.pdf">[Paper]</a></div>
    <div class="links"><a href="https://www.youtube.com/watch?v=JBz4zzQdoso">[Video]</a></div>
    <div class="links"><a href="https://github.com/relh/moves">[Code]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/teaser.png" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
    Given an input image, MOVES produces features (shown using PCA to project to 
    <span style='color:red'>R</span><span style='color:green'>G</span><span style='color:blue'>B</span>) 
    that easily group with ordinary clustering systems and can also be used to associate hands with the objects they hold. The clusters are often sufficient for defining objects, but additional cues such as a box improve them further. At training time, MOVES learns this feature space from direct discriminative training on simple pseudo-labels. While MOVES learns only from objects that hands are actively holding 
    <span style='color:rgb(0,170,151)'>(such as the semi-transparent bag)</span>, 
    we show that it works well on inactive objects as well 
    <span style='color:rgb(235,0,0)'>(such as the milk carton)</span>.
    </p>

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        Our method uses manipulation in video to learn to understand held-objects and hand-object contact. We train a system that takes a single RGB image and produces a pixel-embedding that can be used to answer grouping questions (do these two pixels go together) as well as hand-association questions (is this hand holding that pixel). Rather than painstakingly annotate segmentation masks, we observe people in realistic video data. We show that pairing epipolar geometry with modern optical flow produces simple and effective pseudo-labels for grouping. Given people segmentations, we can further associate pixels with hands to understand contact. Our system achieves competitive results on hand and hand-held object tasks.
    </p>

    <br><br>
    <hr>

    <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/JBz4zzQdoso" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <br><br>
    <hr>

    <h1>Method Overview</h1>
    <img style="width: 80%;" src="./resources/method.png"
         alt="Method overview figure"/>
    <br>
    <p style="width: 80%;">
    As input MOVES accepts an RGB image and produces a 
    \(H \times W \times F\) per-pixel feature embedding using a 
    <span style='color:fcolor'>backbone HRNET 
    denoted \(f(\cdot)\)</span>. Pairs of F-dimensional embeddings from this backbone can be passed to lightweight MLPs 
    <span style='color:gcolor'>\(g(\cdot)\) to assess grouping probability</span> 
    and <span style='color:acolor'>\(a(\cdot)\) to identify hand association</span>, 
    or if the pixels are a hand and an object the hand is holding. Once trained, the MOVES embeddings (here visualized with PCA to map the feature dimension to 
    <span style='color:red'>R</span><span style='color:green'>G</span><span style='color:blue'>B</span>)  
    can be used for: <i>(Clusters)</i> directly applying HDBSCAN 
    to the embeddings produces a good oversegmentation; 
    <i>(Box2Seg)</i>: Given a box, one can produce a more accurate segment; and 
    <i>(Hand Association)</i> Applying \(a\) to a query point and every pixel produces hand-object association (here, to a drawer).</p>
    <a class="links" href="https://github.com/relh/moves">[Code]</a>

    <br><br>
    <hr>

    <h1>Results</h1>
    <img style="width: 80%;" src="./resources/results.png"
         alt="Results figure"/>
<p style="width: 80%;">Results from MOVES, with examples from the EPICK VISOR validation set. <b>Key:</b> Each column shows a different input image. From left to right we show: (<i>Image</i>) the input image; (<i>MOVES Features</i>) a PCA projection of the feature space to 
<span style='color:red'>R</span><span style='color:green'>G</span><span style='color:blue'>B</span>; (<i>Clusters</i>) The clusters found by HDBSCAN applied to the feature space with each cluster visualized with a random color; (<i>Association</i>) The prediction of the association head on the image on one of the hands in the image; (<i>Hands+Held</i>) A Mask of hands and hand-held Objects in the image. The association head usually does a good job of recognizing the objects that hands are holding. <b>Discussion:</b> 
</p>
<div style="width: 80%;">
<ul style="text-align: left;">
    <li>(row 1) although the cabinet door is thin, MOVES recognizes the association between hand and door.</li>
    <li>(row 2) MOVES detects a large mixing bowl.</li>
    <li>(row 3) the transparent glass pan lid is recognized as an object by MOVES despite the stovetop below being visible through it.</li>
    <li>(row 4) the multi-colored hand towel is clustered as separate segments, however the association head helps segment most of the hand towel, showing the complementary nature of pairing an association head with clustering.</li>
    <li>(row 5) the transparent bottle is segmented nicely.</li>
    <li>(row 6) the cutting board is being cleared into the trashcan, but MOVES successfully identifies the board as being the held object.</li>
</ul>
</div>

    <br><br>
    <hr>

    <h1>Poster</h1>
    <img style="width: 80%;" src="./resources/poster.png"
         alt="poster figure"/>
    <br>
    <p style="width: 80%;">
    </p>


    <br><br>
    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Higgins_MOVES_Manipulated_Objects_in_Video_Enable_Segmentation_CVPR_2023_paper.pdf">
            <img class="layered-paper-big" width="100%" src="./resources/paper.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>MOVES: Manipulated Objects in Video Enable Segmentation</h3>
        <p>Richard E. L. Higgins and David F. Fouhey</p>
        <p>In Computer Vision and Pattern Recognition, 2023.</p>
        <pre><code>
        @inproceedings{higgins2023moves,
        title={MOVES: Manipulated Objects in Video Enable Segmentation},
        author={Higgins, Richard EL and Fouhey, David F},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={6334--6343},
        year={2023}
}
</code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p>

    <br><br>
</div>

</body>

</html>
