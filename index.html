<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>This is my paper title</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="Path to my teaser.jpg"/>
  	<meta property="og:title" content="Creative and Descriptive Paper Title." />
  	<meta property="og:description" content="Paper description." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="Creative and Descriptive Paper Title." />
    <meta property="twitter:description"   content="Paper description." />
    <meta property="twitter:image"         content="Path to my teaser.jpg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZHY384EZP7"></script>
    <script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments);}
              gtag('js', new Date());

              gtag('config', 'G-ZHY384EZP7');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        MOVES: Manipulated Objects in Video Enable Segmentation
    </div>

    <div class="venue">
        In Computer Vision and Pattern Recognition 2023
    </div>

    <br><br>

    <div class="author">
        <a href="https://relh.net">Richard E. L. Higgins</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a><sup>1, 2</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>University of Michigan</div>
    <div class="affiliation"><sup>2&nbsp;</sup>New York University</div>

    <br><br>

    <div class="links"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Higgins_MOVES_Manipulated_Objects_in_Video_Enable_Segmentation_CVPR_2023_paper.pdf">[Paper]</a></div>
    <div class="links"><a href="https://www.youtube.com/watch?v=JBz4zzQdoso">[Video]</a></div>
    <div class="links"><a href="https://github.com/relh/moves">[Code]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/teaser.png" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
    Given an input image, MOVES produces features (shown
    using PCA to project to RGB) that easily group with ordinary clustering systems and can also be used to associate hands with the
    objects they hold. The clusters are often sufficient for defining
    objects, but additional cues such as a box further improve them.
    At training time, MOVES learns this feature space from direct
    discriminative training on simple pseudo-labels. While MOVES
    learns only from objects that hands are actively holding (such as
    the semi-transparent bag), we show that it works well on inactive
    objects as well (such as the milk carton).
    </p>

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        Our method uses manipulation in video to learn to understand held-objects and hand-object contact. We train a system that takes a single RGB image and produces a pixel-embedding that can be used to answer grouping questions (do these two pixels go together) as well as hand-association questions (is this hand holding that pixel). Rather than painstakingly annotate segmentation masks, we observe people in realistic video data. We show that pairing epipolar geometry with modern optical flow produces simple and effective pseudo-labels for grouping. Given people segmentations, we can further associate pixels with hands to understand contact. Our system achieves competitive results on hand and hand-held object tasks.
    </p>

    <br><br>
    <hr>

    <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/JBz4zzQdoso" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <br><br>
    <hr>

    <h1>Method Overview</h1>
    <img style="width: 80%;" src="./resources/method.png"
         alt="Method overview figure"/>
    <br>
    <a class="links" href="https://github.com/relh/moves">[Code]</a>

    <br><br>
    <hr>

    <h1>Results</h1>
    <img style="width: 80%;" src="./resources/results.png"
         alt="Results figure"/>

    <br><br>
    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Higgins_MOVES_Manipulated_Objects_in_Video_Enable_Segmentation_CVPR_2023_paper.pdf">
            <img class="layered-paper-big" width="100%" src="./resources/paper.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>MOVES: Manipulated Objects in Video Enable Segmentation</h3>
        <p>Richard E. L. Higgins and David F. Fouhey</p>
        <p>In Computer Vision and Pattern Recognition, 2023.</p>
        <pre><code>
        @inproceedings{higgins2023moves,
        title={MOVES: Manipulated Objects in Video Enable Segmentation},
        author={Higgins, Richard EL and Fouhey, David F},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={6334--6343},
        year={2023}
}
</code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p>

    <br><br>
</div>

</body>

</html>
